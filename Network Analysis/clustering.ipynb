{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import numpy as np\n",
    "import utils.basic_utils as bu\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from folium import Popup\n",
    "from folium import plugins\n",
    "# import matplotlib.cm as cm\n",
    "import branca.colormap as cm\n",
    "import matplotlib.colors as colors\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "# from sklearn.metrics.pairwise import haversine_distances\n",
    "from joblib import Parallel, delayed\n",
    "import cudf \n",
    "from cuml.cluster import DBSCAN\n",
    "import cupy as cp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "name = 'ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cudf.read_csv(f'result/stay_points/stay_points_{name}_updatedpub150.csv')\n",
    "df = df[~df['port_code'].isna()]\n",
    "df = df[df['min_distance']<10]\n",
    "df['country_water_body'] = df['country'] + '_' + df['water_body']\n",
    "labels, uniques = cudf.factorize(df['country_water_body'])\n",
    "df['cluster_label'] = labels\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "len(df['cluster_label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix_gpu(class_coords):\n",
    "    \"\"\"\n",
    "    给定停靠点坐标数组 class_coords (shape=(n,2), 第一列是 latitude, 第二列是 longitude)，\n",
    "    利用 GPU（Cupy）计算各点间的 Haversine 距离（单位：千米），返回一个 n x n 的距离矩阵（NumPy 数组）。\n",
    "    \"\"\"\n",
    "    # 将输入转换为 cupy 数组\n",
    "    coords_cp = cp.asarray(class_coords)  # shape: (n, 2)\n",
    "    \n",
    "    # 分别提取纬度和经度。注意：haversine_distance_gpu 的参数顺序是 (lon1, lat1, lon2, lat2)\n",
    "    lat = coords_cp[:, 0]  # shape: (n,)\n",
    "    lon = coords_cp[:, 1]  # shape: (n,)\n",
    "    \n",
    "    # 将纬度和经度重塑为列向量和行向量以利用广播机制\n",
    "    lat_i = lat.reshape(-1, 1)   # shape: (n, 1)\n",
    "    lon_i = lon.reshape(-1, 1)   # shape: (n, 1)\n",
    "    lat_j = lat.reshape(1, -1)   # shape: (1, n)\n",
    "    lon_j = lon.reshape(1, -1)   # shape: (1, n)\n",
    "    \n",
    "    # 调用你封装好的 GPU Haversine 距离函数，注意传入参数顺序\n",
    "    distance_matrix_gpu = bu.haversine_distance_gpu(lon_i, lat_i, lon_j, lat_j)\n",
    "    \n",
    "    # 将结果从 GPU (cupy 数组) 转换为 NumPy 数组返回\n",
    "    distance_matrix = cp.asnumpy(distance_matrix_gpu)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/328 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [00:11<00:00, 27.58it/s] \n"
     ]
    }
   ],
   "source": [
    "# 最大采样数量\n",
    "max_points = 10000\n",
    "\n",
    "eps = 200\n",
    "min_samples = 50\n",
    "\n",
    "offset = df['cluster_label'].max()\n",
    "df['cluster'] = df['cluster_label'].copy()\n",
    "\n",
    "for label in tqdm(df['cluster_label'].to_pandas().unique()):\n",
    "    mask = df['cluster'] == label\n",
    "    # 从当前组中获取所有点的经纬度（转换为 pandas 数组）\n",
    "    class_coords = df[mask][['latitude', 'longitude']].to_pandas().values\n",
    "    num_points = len(class_coords)\n",
    "    \n",
    "    if num_points <= 1:\n",
    "        df.loc[mask, 'cluster'] = -1\n",
    "        continue\n",
    "\n",
    "    # 如果点数超过 max_points，则随机采样\n",
    "    if num_points > max_points:\n",
    "        # 随机选取 max_points 个索引，不放回抽样\n",
    "        sample_idx = np.random.choice(num_points, max_points, replace=False)\n",
    "        sampled_coords = class_coords[sample_idx]\n",
    "        # 对采样点计算距离矩阵（分块函数已封装在 compute_distance_matrix_gpu 内）\n",
    "        distance_matrix_sampled = compute_distance_matrix_gpu(sampled_coords)\n",
    "        # DBSCAN 聚类（采样点）\n",
    "        dbscan_cluster_sampled = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed').fit_predict(distance_matrix_sampled)\n",
    "        # 创建一个全 -1 的聚类结果数组，长度为原组点数\n",
    "        dbscan_cluster = -1 * np.ones(num_points, dtype=int)\n",
    "        # 将采样点的聚类结果放入对应位置\n",
    "        dbscan_cluster[sample_idx] = dbscan_cluster_sampled\n",
    "    else:\n",
    "        # 如果点数不超过 max_points，直接计算整个距离矩阵\n",
    "        distance_matrix = compute_distance_matrix_gpu(class_coords)\n",
    "        dbscan_cluster = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed').fit_predict(distance_matrix)\n",
    "    \n",
    "    # 将 DBSCAN 聚类结果转换为 cupy 数组，并进行调整：\n",
    "    dbscan_cluster_cp = cp.array(dbscan_cluster)\n",
    "    # 对于非 -1 的聚类结果，如果为 0 则保留原 cluster，否则加上 offset\n",
    "    dbscan_cluster_cp = cp.where(dbscan_cluster_cp == -1, -1, cp.where(dbscan_cluster_cp == 0,\n",
    "                                                    dbscan_cluster_cp,\n",
    "                                                    dbscan_cluster_cp + offset))\n",
    "    # 更新原数据中对应点的聚类结果\n",
    "    df.loc[mask, 'cluster'] = cp.asnumpy(cp.where(dbscan_cluster_cp == -1,\n",
    "                                                   -1,\n",
    "                                                   cp.where(dbscan_cluster_cp == 0,\n",
    "                                                            df.loc[mask, 'cluster'],\n",
    "                                                            dbscan_cluster_cp)))\n",
    "    # 更新 offset: 如果本组中 DBSCAN 的最大值为 -1（全部离群），offset 保持不变，否则增加当前组的最大聚类编号\n",
    "    if dbscan_cluster.max() != -1:\n",
    "        offset += dbscan_cluster.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "266\n"
     ]
    }
   ],
   "source": [
    "print(len(df['cluster_label'].unique()))\n",
    "print(len(df['cluster'].unique()))\n",
    "\n",
    "df.to_csv(f'result/stay_points/stay_points_{name}_cluster.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 画离群点图\n",
    "m = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=5,tiles='cartodbpositron')\n",
    "colors = ['blue', 'green', 'purple', 'orange', 'darkred', 'lightred', 'beige', 'darkblue', 'darkgreen', 'cadetblue', 'darkpurple', 'pink', 'lightblue', 'lightgreen', 'gray', 'black', 'lightgray']\n",
    "for idx, row in df.to_pandas().iterrows():\n",
    "    color = 'red' if row['cluster'] == -1 else colors[row['cluster'] % len(colors)]\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=2,\n",
    "        color=color, \n",
    "        fill=True,\n",
    "        # popup=f'Label:{row[\"cluster\"]},Speed_max: {row[\"speed_max\"]}'\n",
    "        popup=f'Label:{row[\"cluster\"]}'\n",
    "    ).add_to(m)\n",
    "m.add_child(folium.LatLngPopup())\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存聚类多边形数据到: result/clusters/cl_01-03.feather\n",
      "已保存聚类多边形数据到: result/clusters/cl_01-03.csv\n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "import geopandas as gpd\n",
    "\n",
    "# 读取并处理数据（已聚类处理后的结果）\n",
    "df = cudf.read_csv(f'result/stay_points/stay_points_{name}_cluster.csv')\n",
    "df = df[df['cluster'] != -1]\n",
    "df['cluster'] = cudf.factorize(df['cluster'])[0]\n",
    "df = df.to_pandas()\n",
    "\n",
    "# 1. 将停靠点数据转换为 GeoDataFrame，设置坐标系为 WGS84 (EPSG:4326)\n",
    "gdf_points = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df['longitude'], df['latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# 2. 按照聚类标签分组，对每个聚类构造凸包作为该聚类的多边形\n",
    "polygons_list = []\n",
    "for cluster_label, group in gdf_points.groupby('cluster'):\n",
    "    if len(group) > 0:\n",
    "        union_geom = group.unary_union\n",
    "        convex_hull = union_geom.convex_hull\n",
    "        ship_cnts = len(group)\n",
    "\n",
    "        # 合并 country_water_body 字段，按频率降序排序并剔除出现频率小于最大频率一半的元素\n",
    "        cw_counts = group['country_water_body'].value_counts()\n",
    "        max_freq_cw = cw_counts.max()  # 最大频率\n",
    "        cw_counts = cw_counts[cw_counts >= max_freq_cw / 2]  # 只保留频率大于等于最大频率一半的元素\n",
    "        cw = ', '.join(cw_counts.index.tolist())\n",
    "\n",
    "        # 合并 nearest_port 字段，按频率降序排序并剔除出现频率小于最大频率一半的元素\n",
    "        port_counts = group['nearest_port'].value_counts()\n",
    "        max_freq_ports = port_counts.max()  # 最大频率\n",
    "        port_counts = port_counts[port_counts >= max_freq_ports / 2]  # 只保留频率大于等于最大频率一半的元素\n",
    "        ports = ', '.join(port_counts.index.tolist())\n",
    "        \n",
    "        polygons_list.append({\n",
    "            'label': cluster_label,\n",
    "            'geometry': convex_hull,\n",
    "            'ship_cnts': ship_cnts,\n",
    "            'country_water_body': cw,\n",
    "            'nearest_ports': ports\n",
    "        })\n",
    "\n",
    "# 3. 创建一个 GeoDataFrame 保存所有聚类生成的多边形\n",
    "gdf_polygons = gpd.GeoDataFrame(polygons_list, crs=\"EPSG:4326\")\n",
    "gdf_polygons.sort_values(by='label', inplace=True)\n",
    "\n",
    "# 4. 保存结果到 Feather 文件\n",
    "gdf_polygons.to_feather(f'result/clusters/cl_{name}.feather')\n",
    "print(f'已保存聚类多边形数据到: result/clusters/cl_{name}.feather')\n",
    "\n",
    "# 5. 同时保存到 CSV 文件\n",
    "gdf_polygons.to_csv(f'result/clusters/cl_{name}.csv', index=False)\n",
    "print(f'已保存聚类多边形数据到: result/clusters/cl_{name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
